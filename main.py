import numpy as np
import torch
import os
import time

from utils import parse_args, set_seed_everywhere, ReplayBuffer, make_batch_env, make_musik_learner, make_psdp_learner

from algs.musik_policy import MusIK_Policy
from algs.psdp_policy import PSDP_Policy


os.environ["OMP_NUM_THREADS"] = "1"

     
def train(learners, buffers, h, H):

    reg_loss = learners.update(buffers, h)
    learners.save(h, H)
    #if queue is not None:
    #    queue.put([h, reg_loss])
    #else:
    return reg_loss


def evaluate(env, agent, args):
    returns = np.zeros((args.num_eval,1))
    obs = env.reset()
    for h in range(args.horizon):
        action = agent.act_batch(obs, h)
        next_obs, reward, done, _ = env.step(action)
        obs = next_obs
        returns += reward

    return np.mean(returns)

def partial_eval(policy_cover, env, agent, args, state_dim, h):
    I = np.random.randint(0, state_dim, env.num_envs)
    obs = policy_cover.roll_in_musikpol(I, h, env)
    
    ## Take random action at Layer t
    R = np.zeros((env.num_envs,1))
    
    ## Roll with PSDP policy 
    for l in range(h, args.horizon):
        action = agent.act_batch(obs, l)  
        obs, r, _, _ = env.step(action) 
        R += r
        
    return np.mean(R)
    

def main(args):
    
   # args.temp_path = args.temp_path  + "/seed_{}_horizon_{}_episodes_{}_hidden_dim_{}_updates_{}_batch_size_{}".format(args.seed,args.horizon,args.num_episodes,args.hidden_dim,args.rep_num_feature_update,args.batch_size)
    args.temp_path = args.temp_path  + "/seed={}_horizon={}".format(args.seed,args.horizon)

    if not os.path.exists(args.temp_path):
        os.makedirs(args.temp_path)
    set_seed_everywhere(args.seed)
    device = torch.device("cuda")  if torch.cuda.is_available() else torch.device("cpu") 
    
    # Setting problem variables
    num_envs = int(args.num_episodes/args.horizon)
    env, eval_env = make_batch_env(args, num_envs)
    if args.state_dim is not None:
        state_dim = args.state_dim
    else:
        state_dim = env.state_dim
    num_actions = env.action_space.n
    input_batch_size = args.batch_size
    args.batch_size = min(num_envs, args.batch_size)
    
    # MusIK learner 
    musik_learners = make_musik_learner(env, state_dim, device, args)
    
    # MusIK policies
    policy_cover = MusIK_Policy(env.observation_space.shape[0],
                 state_dim, 
                 env.action_dim,
                 args.horizon,
                 device,
                 musik_learners
                 )
    
    
    # Buffers to store trajectory data generated by musik
    buffers = []    
    for h in range(args.horizon):
        buffers.append(
                    ReplayBuffer(env.observation_space.shape, 
                                 env.action_space.n, 
                                 num_envs, 
                                 args.batch_size, 
                                 device)
            )
    
    # PSDP stuff 
    num_envs_psdp = int(args.num_episodes_psdp/args.horizon)
    args.batch_size_psdp = min(num_envs_psdp, args.batch_size)
    #assert(num_envs_psdp>=2000)
    
    
    # PSDP learner 
    psdp_learners = make_psdp_learner(env, state_dim, device, args)
    
    # MusIK policies
    psdp_policy = PSDP_Policy(env.observation_space.shape[0],
                 state_dim, 
                 env.action_dim,
                 args.horizon,
                 device,
                 psdp_learners
                 )
    
        
    # Buffers to store datafor PSDP
    buffers_PSDP = []    
    for h in range(args.horizon):
        buffers_PSDP.append(
                    ReplayBuffer(env.observation_space.shape, 
                                 env.action_space.n, 
                                 num_envs_psdp, 
                                 args.batch_size_psdp, 
                                 device)
            )
    
    
    # Initializing 
    counts = np.zeros((args.horizon,3),dtype=np.int32)
    exploration_time = time.time()
    
    ##### MusIK Part #####
    for h in range(1, args.horizon):
        ## Uncomment the following for the option to load a MusIK model
        try:
            musik_learners[h].load(h, args.horizon) 
            print("Loaded MusIK model for layer {}".format(h))
            continue
        except:
            if h == 1:
                print("Learning a policy cover for layer {}".format(h))
        
        # Pick a policy uniformly at random to roll-in with to Layer t  
        I = np.random.randint(0, state_dim, num_envs) 
        I_target = I.reshape((num_envs,1))
        #I = i * np.ones(num_envs, dtype=int)
            
        ## Get the list of target indices
        obs_t = policy_cover.roll_in_musikpol(I, h-1, env)
        
        ## Take random action at Layer t
        action_t = np.random.randint(0, num_actions, num_envs)
        obs_h, _, _, _ = env.step(action_t)
        
        buffers[h].add_batch(obs_t,action_t, obs_h, num_envs, target_states = I_target)
        reg_loss = train(musik_learners[h], buffers[h], h, args.horizon) 
        print('Reg loss =', reg_loss)
    
        # Evaluating policy cover at layer h
        counts[h] = policy_cover.eval_cover(h, eval_env, args, state_dim)
        print("The abstract state counts at layer h={} are {}".format(h,counts[h]))
        if counts[h][0] ==0:
            break
        
    print('Time to learn a policy cover is %f' %(time.time()- exploration_time))
    np.save("{}/counts".format(args.temp_path), counts)
    
    print("Finished learning a policy cover")
    print("Now, planning using PSDP")
    
    
    ### Planning part ###
    env.num_envs = num_envs_psdp
    
    ##### PSDP Part #####
    planning_time = time.time()
    ##### Learning PSDP policy #####
    for h in reversed(range(args.horizon)):     
        ## Uncomment the following for the option to load a PSDP model
        try:
            psdp_learners[h].load(h, args.horizon) 
            print("Loaded PSDP model for layer {}".format(h))
            continue
        except:
            if h == 1:
                print("Learning a policy cover for layer {}".format(h))
        
        I = np.random.randint(0, state_dim, num_envs_psdp) 
        
        # Roll-in with MusIK policies up to layer h
        obs_h = policy_cover.roll_in_musikpol(I, h, env)
        
        ## Take random action at Layer t
        R = np.zeros((num_envs_psdp,1))
        action_h = np.random.randint(0, num_actions, num_envs_psdp)
        obs, r, _, _ = env.step(action_h)
        R += r
        
        ## Roll with PSDP policy 
        for l in range(h+1, args.horizon):
            action = psdp_policy.act_batch(obs, l)  
            obs, r, _, _ = env.step(action) 
            R += r
        
        buffers_PSDP[h].add_batch(obs_h, action_h, obs_h, num_envs_psdp, rewards=R)

        reg_loss = train(psdp_learners[h], buffers_PSDP[h], h, args.horizon) 
        print('Reg loss =', reg_loss)
        
        mean_reward = partial_eval(policy_cover, eval_env, psdp_policy, args, state_dim, h)
        print("The future mean reward at layer h={} is {}".format(h, mean_reward))
        
    print('Planning time using PSDP is %f' %(time.time()- planning_time))
    mean_reward = evaluate(eval_env, psdp_policy, args)
    string = "seed={}, horizon={}, episodes={}, hidden_dim={}, updates={}, batch_size={}".format(args.seed,args.horizon,args.num_episodes,args.hidden_dim,args.rep_num_feature_update,input_batch_size)
    print('The mean reward is {} for the setting where {}'.format(mean_reward, string))
    

if __name__ == '__main__':

    args = parse_args()
    if args.variable_latent:
        project_name = "bmdp_et{}".format(str(args.env_temperature))
    elif args.dense:
        project_name = "bmdp_dense_h{}".format(str(args.horizon))
    else:
        project_name = "bmdp_h{}".format(args.horizon)
    
    main(args) 
    

    ## Example of how to run a hyperparamter sweep
    # args.hidden_dim = 100
    # args.num_episodes = 1100000
    # args.seed = 0
    # args.horizon = 100 
    # args.batch_size = 4096
    # args.rep_num_feature_update = 128
    # for hidden_dim in 100,200,400:
    #     for batch_size in 4096, 8196:
    #         args.temp_path = "temp0"
    #         args.hidden_dim = hidden_dim
    #         args.batch_size = batch_size
    #         #print("seed is {}".format(seed))
    #         main(args)





